# -*- coding: utf-8 -*-
"""
Streamlit aplikace pro vyÄiÅ¡tÄ›nÃ­ produktovÃ©ho XML feedu z Heureka.cz
pro Kaufland marketplace. Sanitizuje HTML v popisech produktÅ¯.
"""

# Project: base_kaufland
import re
from typing import Optional

import streamlit as st
from bs4 import BeautifulSoup, NavigableString
from lxml import etree

# ---------------------------------------------------------------------------
# Konfigurace ÄiÅ¡tÄ›nÃ­
# ---------------------------------------------------------------------------

# Tagy zakÃ¡zanÃ© Kauflandem â€“ budou kompletnÄ› odstranÄ›ny
FORBIDDEN_TAGS = {"img", "iframe", "script", "object", "video", "style", "form"}

# PovolenÃ© tagy pro formÃ¡tovÃ¡nÃ­ (ostatnÃ­ se odstranÃ­ nebo pÅ™evedou na text)
ALLOWED_TAGS = {"p", "b", "strong", "ul", "li", "br", "h1", "h2", "h3", "h4", "h5", "h6"}

# KlÃ­ÄovÃ¡ slova: pokud jsou v okolÃ­ <img>, odstranÃ­ se i pÅ™ilehlÃ½ text/paragraf
IMAGE_CAPTION_KEYWORDS = [
    "velikostnÃ­ tabulka",
    "tabulka velikostÃ­",
    "rozmÄ›ry",
    "viz foto",
    "viz obrÃ¡zek",
    "viz obrÃ¡zek nÃ­Å¾e",
    "viz foto nÃ­Å¾e",
    "velikostnÃ­ tabulka viz",
    "rozmÄ›ry viz",
]


def _normalize_text(text: str) -> str:
    """Normalizuje text pro porovnÃ¡nÃ­ (lowercase, vÃ­ce mezer na jednu)."""
    if not text or not text.strip():
        return ""
    return " ".join(re.split(r"\s+", text.lower().strip()))


def _text_contains_any_keyword(text: str) -> bool:
    """VrÃ¡tÃ­ True, pokud text obsahuje alespoÅˆ jedno klÃ­ÄovÃ© slovo."""
    normalized = _normalize_text(text)
    if not normalized:
        return False
    return any(kw in normalized for kw in IMAGE_CAPTION_KEYWORDS)


def _get_element_text(element) -> str:
    """VrÃ¡tÃ­ ÄistÃ½ text elementu (bez vnoÅ™enÃ½ch tagÅ¯)."""
    if element is None:
        return ""
    if isinstance(element, NavigableString):
        return str(element).strip()
    return element.get_text(separator=" ", strip=True) if hasattr(element, "get_text") else ""


def _remove_image_and_caption_blocks(soup: BeautifulSoup) -> None:
    """
    Pro kaÅ¾dÃ½ <img> zkontroluje okolÃ­ (pÅ™edchozÃ­/nÃ¡sledujÃ­cÃ­ element, rodiÄ).
    Pokud okolnÃ­ text obsahuje klÃ­ÄovÃ¡ slova, odstranÃ­ i tento text/paragraf.
    Nakonec odstranÃ­ vÅ¡echny <img>.
    """
    to_decompose = set()  # id(obj) pro elementy k odstranÄ›nÃ­

    for img in soup.find_all("img"):
        img_id = id(img)
        to_decompose.add(img_id)

        parent = img.parent
        prev_sibling = img.find_previous_sibling()
        next_sibling = img.find_next_sibling()

        # Text rodiÄe (bez obsahu tohoto img)
        parent_text = ""
        if parent and parent.name:
            parent_text = _get_element_text(parent)

        # Pokud rodiÄ obsahuje klÃ­ÄovÃ© slovo, odstranÃ­me celÃ½ rodiÄ (vÄetnÄ› img)
        if parent and parent.name and _text_contains_any_keyword(parent_text):
            to_decompose.add(id(parent))
            continue

        # PÅ™edchozÃ­ sourozenec
        if prev_sibling and getattr(prev_sibling, "name", None):
            prev_text = _get_element_text(prev_sibling)
            if _text_contains_any_keyword(prev_text):
                to_decompose.add(id(prev_sibling))

        # NÃ¡sledujÃ­cÃ­ sourozenec
        if next_sibling and getattr(next_sibling, "name", None):
            next_text = _get_element_text(next_sibling)
            if _text_contains_any_keyword(next_text):
                to_decompose.add(id(next_sibling))

    # OdstranÄ›nÃ­ v poÅ™adÃ­: nejdÅ™Ã­v vÄ›tÅ¡Ã­ kontejnery (rodiÄe), pak img
    # Provedeme decompose v jednom prÅ¯chodu â€“ sbÃ­rÃ¡me objekty
    elements_to_remove = []
    for tag in soup.find_all(True):
        if id(tag) in to_decompose:
            elements_to_remove.append(tag)

    # Odstranit od rodiÄÅ¯ k potomkÅ¯m (menÅ¡Ã­ hloubka = dÅ™Ã­v), aby pÅ™i odstranÄ›nÃ­ rodiÄe
    # neporuÅ¡il odkaz na potomka
    def depth(elem):
        d = 0
        while getattr(elem, "parent", None) and getattr(elem.parent, "name", None):
            elem = elem.parent
            d += 1
        return d

    for elem in sorted(elements_to_remove, key=depth):
        try:
            if elem.parent is not None:
                elem.decompose()
        except Exception:
            pass


def _remove_forbidden_tags(soup: BeautifulSoup) -> None:
    """OdstranÃ­ vÅ¡echny zakÃ¡zanÃ© tagy (obsah i tag)."""
    for tag_name in FORBIDDEN_TAGS:
        for tag in soup.find_all(tag_name):
            tag.decompose()


def _strip_disallowed_tags(soup: BeautifulSoup) -> None:
    """
    Tagy mimo ALLOWED_TAGS odstranÃ­ â€“ samotnÃ½ tag zmizÃ­, text zÅ¯stane (unwrap).
    """
    for tag in soup.find_all(True):
        if tag.name.lower() not in ALLOWED_TAGS:
            tag.unwrap()


def clean_html_description(html_content: Optional[str]) -> str:
    """
    VyÄistÃ­ HTML popis podle pravidel Kaufland marketplace.

    - OdstranÃ­ zakÃ¡zanÃ© tagy: img, iframe, script, object, video, style, form.
    - PÅ™ed odstranÄ›nÃ­m img zkontroluje okolÃ­; pokud obsahuje klÃ­ÄovÃ¡ slova
      (napÅ™. â€velikostnÃ­ tabulkaâ€œ, â€viz fotoâ€œ), odstranÃ­ i pÅ™ilehlÃ½ text/paragraf.
    - PonechÃ¡ pouze povolenÃ© formÃ¡tovÃ¡nÃ­: p, b, strong, ul, li, br, h1â€“h6.

    :param html_content: Å˜etÄ›zec s HTML (mÅ¯Å¾e bÃ½t prÃ¡zdnÃ½ nebo None).
    :return: VyÄiÅ¡tÄ›nÃ½ HTML Å™etÄ›zec v UTF-8.
    """
    if html_content is None or not isinstance(html_content, str):
        return ""

    text = html_content.strip()
    if not text:
        return ""

    try:
        soup = BeautifulSoup(text, "lxml")
    except Exception:
        soup = BeautifulSoup(text, "html.parser")

    # 1) InteligentnÃ­ odstranÄ›nÃ­ obrÃ¡zkÅ¯ a popiskÅ¯
    _remove_image_and_caption_blocks(soup)

    # 2) OdstranÄ›nÃ­ ostatnÃ­ch zakÃ¡zanÃ½ch tagÅ¯
    _remove_forbidden_tags(soup)

    # 3) Ponechat jen povolenÃ© tagy (ostatnÃ­ unwrap)
    _strip_disallowed_tags(soup)

    result = str(soup)
    # Odstranit pÅ™Ã­padnÃ© obalovÃ© tagy, kterÃ© BeautifulSoup pÅ™idal (html, body)
    for wrapper in ("html", "body", "[document]"):
        if result.startswith(f"<{wrapper}") or result.startswith(f"<{wrapper}>"):
            soup2 = BeautifulSoup(result, "html.parser")
            body = soup2.find("body") or soup2.find("html")
            if body:
                result = "".join(str(c) for c in body.children)
            break

    return result.strip() or ""


# ---------------------------------------------------------------------------
# ZpracovÃ¡nÃ­ XML
# ---------------------------------------------------------------------------

DESCRIPTION_TAGS = ("DESCRIPTION", "LONG_DESCRIPTION")


def process_heureka_xml(xml_bytes: bytes) -> bytes:
    """
    NaÄte Heureka XML, projde kaÅ¾dÃ½ SHOPITEM, vyÄistÃ­ DESCRIPTION a LONG_DESCRIPTION
    a vrÃ¡tÃ­ novÃ½ XML jako bytes (UTF-8).

    :param xml_bytes: Obsah XML souboru (libovolnÃ© kÃ³dovÃ¡nÃ­, preferovÃ¡no UTF-8).
    :return: VyÄiÅ¡tÄ›nÃ½ XML jako bytes v UTF-8.
    :raises: ValueError pÅ™i nevalidnÃ­m XML.
    """
    # DekÃ³dovÃ¡nÃ­ na Å™etÄ›zec (UTF-8 nebo fallback)
    try:
        xml_str = xml_bytes.decode("utf-8")
    except UnicodeDecodeError:
        try:
            xml_str = xml_bytes.decode("cp1250")
        except UnicodeDecodeError:
            xml_str = xml_bytes.decode("utf-8", errors="replace")

    # ParsovÃ¡nÃ­ XML
    try:
        root = etree.fromstring(xml_str.encode("utf-8"))
    except etree.XMLSyntaxError as e:
        raise ValueError(f"NevalidnÃ­ XML: {e}") from e

    # NalezenÃ­ vÅ¡ech SHOPITEM (bez ohledu na namespace)
    shopitems = [
        e for e in root.iter()
        if (e.tag or "").replace("}", "").split("{")[-1].upper() == "SHOPITEM"
    ]
    if not shopitems:
        raise ValueError("V XML se nenaÅ¡el Å¾Ã¡dnÃ½ element SHOPITEM.")

    for item in shopitems:
        for elem in item:
            tag_local = (elem.tag or "").replace("}", "").split("{")[-1].upper()
            if tag_local not in DESCRIPTION_TAGS:
                continue
            # SbÄ›r celÃ©ho obsahu (text + vnoÅ™enÃ© elementy) jako HTML
            raw_parts = [elem.text or ""]
            for child in elem:
                raw_parts.append(etree.tostring(child, encoding="unicode", method="html"))
                raw_parts.append(child.tail or "")
            raw_html = "".join(raw_parts).strip()
            if not raw_html:
                continue
            cleaned = clean_html_description(raw_html)
            elem.text = cleaned
            for child in list(elem):
                elem.remove(child)

    return etree.tostring(
        root,
        encoding="utf-8",
        xml_declaration=True,
        pretty_print=True,
        method="xml",
        standalone=False,
    )


# ---------------------------------------------------------------------------
# Streamlit UI
# ---------------------------------------------------------------------------

def main() -> None:
    st.set_page_config(
        page_title="Heureka â†’ Kaufland XML ÄistiÄka",
        page_icon="ğŸ§¹",
        layout="centered",
    )
    st.title("VyÄiÅ¡tÄ›nÃ­ produktovÃ©ho XML feedu")
    st.markdown(
        "Nahrajte XML feed ve formÃ¡tu **Heureka.cz**. Aplikace vyÄistÃ­ popisy produktÅ¯ "
        "podle poÅ¾adavkÅ¯ **Kaufland marketplace** a pÅ™ipravÃ­ soubor ke staÅ¾enÃ­."
    )

    uploaded_file = st.file_uploader(
        "Vyberte XML soubor (Heureka)",
        type=["xml"],
        accept_multiple_files=False,
    )

    if uploaded_file is None:
        st.info("Pro zaÄÃ¡tek nahrajte XML soubor.")
        return

    raw_bytes = uploaded_file.read()

    if not raw_bytes.strip():
        st.error("Soubor je prÃ¡zdnÃ½.")
        return

    with st.spinner("ZpracovÃ¡vÃ¡m XML a ÄistÃ­m popisyâ€¦"):
        try:
            result_bytes = process_heureka_xml(raw_bytes)
        except ValueError as e:
            st.error(str(e))
            return
        except Exception as e:
            st.exception(e)
            return

    st.success("XML bylo ÃºspÄ›Å¡nÄ› zpracovÃ¡no.")

    out_name = "kaufland_feed_cleaned.xml"
    st.download_button(
        label="StÃ¡hnout vyÄiÅ¡tÄ›nÃ½ XML soubor",
        data=result_bytes,
        file_name=out_name,
        mime="application/xml",
    )


if __name__ == "__main__":
    main()
